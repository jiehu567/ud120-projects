{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning to analyze Enron Dataset\n",
    "**Udacity Nanodegree P5 project**\n",
    "\n",
    "*Author: Jie Hu,  jie.hu.ds@gmail.com*\n",
    "\n",
    "------------\n",
    "\n",
    "## 1. Abstract\n",
    "\n",
    "\n",
    "Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "Structure of this report:\n",
    "\n",
    "\n",
    "Furthermore, I will go through Email data and try to extract significant features (words).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Dataset\n",
    "\n",
    "2.\tWhat features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data \n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Summarise data\n",
    "\n",
    "1) Overall: Dimension, number and type of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 21)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of data points and number of features of each person\n",
    "len(data_dict.viewkeys()), len(data_dict[data_dict.keys()[1]].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data of 146 persons, for each person there're 21 features, including poi (Label, person of interest) which we are interested in. The other 20 features are 14 finance features, like 'salary', 'total_payments', 'loan_advances', 'bonus', etc. and 6 email features, like 'to_messages', 'email_address', 'from_poi_to_this_person'. Now let's take a look at each and select the features most intuitively related to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Each feature\n",
    "\n",
    "** - poi -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I will use this to extract feature values of a person\n",
    "import numpy as np\n",
    "\n",
    "def feature_extract(feature_name, data_dict = data_dict):\n",
    "    \n",
    "    all_person_names = data_dict.keys()\n",
    "    feature_lst = []\n",
    "    \n",
    "    for name in all_person_names:\n",
    "        feature_lst.append(data_dict[name][feature_name])\n",
    "    \n",
    "    return feature_lst\n",
    "\n",
    "# summary functions\n",
    "\n",
    "def summarise_num_data(num_lst):\n",
    "    \n",
    "    na_count = 0\n",
    "    value_lst = []\n",
    "    for ii in range(len(num_lst)):\n",
    "        if num_lst[ii] == 'NaN':\n",
    "            na_count +=1\n",
    "        else:\n",
    "            value_lst.append(num_lst[ii])\n",
    "    \n",
    "    mu = np.mean(value_lst)\n",
    "    std_dev = np.std(value_lst)\n",
    "    \n",
    "    print \"Mean     : %s\" % \"{:,.2f}\".format(round(mu, 2))\n",
    "    print \"Std. Dev.: %s\" % \"{:,.2f}\".format(round(std_dev, 2))\n",
    "    print \"Count of NA values: %d\" % na_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, '12.33%')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi = feature_extract('poi', data_dict)\n",
    "sum(poi), \"{0:2.2f}%\".format(round(sum(poi)/1.46, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'poi', person of interest, is a bool indicator showing whether this person is commited of fraud. There are totally 18 poi persons, taking up 12.33% of our dataset. No missing values, but it's a pretty unbalanced dataset with big difference of label proportions! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** - salary -**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean     : 562,194.29\n",
      "Std. Dev.: 2,702,034.65\n",
      "Count of NA values: 51\n"
     ]
    }
   ],
   "source": [
    "salary = feature_extract('salary', data_dict)\n",
    "summarise_num_data(salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salary of the person, by US dollar, has a flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "salary = salary.remove('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meaning and type of each features\n",
    "distribution\n",
    "missing values?\n",
    "How features are probably relevant to our question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create new feature\n",
    "\n",
    "\n",
    "4) Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning\n",
    "\n",
    "(list pro and cons of each step)\n",
    "1) Data type transform\n",
    "2) Deal with missing value, remove feature if too many value and meanwhile it's not good indicator\n",
    "3) Remove less useful features\n",
    "4) Imputation\n",
    "Now we have cleaned dataset\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Transforming\n",
    "1) Correlation\n",
    "2) Rescale, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Further explore\n",
    "\n",
    "### 3.1 Unsupervised Learning explore\n",
    "\n",
    "PCA, pickup the principle components, for Kmeans, SVM\n",
    "\n",
    "Check if POI and non-POI have pattern within each cluster\n",
    "\n",
    "### 3.2 Initial dance with Supervised Learning\n",
    "\n",
    "*Default Settings*\n",
    "\n",
    "1) Decision Tree\n",
    "2) SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improvements of algorithms\n",
    "\n",
    "### 4.1 problems of above methods\n",
    "overfitting\n",
    "\n",
    "### 4.2 Validation\n",
    "\n",
    "### 4.3 \n",
    "\n",
    "### 4.4 Tune parameters\n",
    "Best performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate algorithms\n",
    "\n",
    "Compare matrix: precision, recall rate, F1 score, ROC Curve\n",
    "Each include introduction to the concept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Email Explore\n",
    "Above I only explore finance part, however, there's email dataset which I can apply to process.\n",
    "\n",
    "### 6.1 Email explore\n",
    "\n",
    "- Top receipt from POI\n",
    "- Words most freqently used by POI in email\n",
    "Include pre-processing of email, like stem words, tfidf of all documents\n",
    "- PCA and vectorize words, choose 2 vectors, kmeans and then color by POI and non-POI\n",
    "\n",
    "### 6.2 Add email features (words frequency) to finance data and predict POI\n",
    "\n",
    "- Decision Tree\n",
    "Tune\n",
    "\n",
    "- SVM\n",
    "Tune\n",
    "\n",
    "### 6.3 Evaluate algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_py2]",
   "language": "python",
   "name": "conda-env-ipykernel_py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
